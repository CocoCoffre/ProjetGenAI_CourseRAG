import streamlit as st
import os
import tempfile
from dotenv import load_dotenv

# --- Tes Imports (Adapt√©s pour Streamlit Cloud) ---
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate

# Note : 'langchain_classic' est un dossier local, sur le cloud on utilise la lib officielle :
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# --- Config ---
st.set_page_config(page_title="Projet RAG √âtudiant", page_icon="üéì")
load_dotenv()

# --- Fonctions Logiques (Remplacement de tes modules locaux) ---

def process_documents(uploaded_files):
    """
    Remplace 'download_data' et 'clean_transform'.
    G√®re le stockage temporaire et le chargement via PyPDFLoader.
    """
    documents = []
    for file in uploaded_files:
        # Cr√©ation fichier temporaire pour PyPDFLoader
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(file.getvalue())
            tmp_path = tmp_file.name
        
        try:
            loader = PyPDFLoader(tmp_path)
            docs = loader.load()
            documents.extend(docs)
        finally:
            os.remove(tmp_path) # Nettoyage
    return documents

def build_vector_store(documents):
    """
    Remplace ton module 'build_embeddings'.
    Split le texte et cr√©e la base FAISS.
    """
    # Split
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    splits = text_splitter.split_documents(documents)
    
    # Embeddings
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
    # Vector Store
    vectorstore = FAISS.from_documents(splits, embeddings)
    return vectorstore

def get_rag_chain(vectorstore):
    """Configuration de la cha√Æne RAG (LCEL)"""
    
    # R√©cup√©ration API Key
    groq_api_key = st.secrets.get("GROQ_API_KEY")
    if not groq_api_key:
        st.error("Cl√© API Groq manquante dans les secrets Streamlit.")
        st.stop()

    # LLM
    llm = ChatGroq(
        groq_api_key=groq_api_key,
        model_name="llama3-70b-8192",
        temperature=0.3
    )

    # Prompt
    prompt = ChatPromptTemplate.from_template("""
    R√©ponds √† la question en te basant uniquement sur le contexte fourni ci-dessous.
    Si tu ne trouves pas la r√©ponse dans le contexte, dis simplement que tu ne sais pas.
    
    <context>
    {context}
    </context>

    Question: {input}
    """)

    # Cha√Ænes (LCEL standard)
    document_chain = create_stuff_documents_chain(llm, prompt)
    retrieval_chain = create_retrieval_chain(vectorstore.as_retriever(), document_chain)
    
    return retrieval_chain

# --- Application Streamlit ---

def main():
    st.title("üéì Assistant RAG √âtudiant")

    # Initialisation de la session
    if "vectorstore" not in st.session_state:
        st.session_state.vectorstore = None

    # Sidebar : Chargement des donn√©es
    with st.sidebar:
        st.header("1. Chargement des Cours")
        uploaded_files = st.file_uploader("Upload PDF", type="pdf", accept_multiple_files=True)
        
        if st.button("Traiter les documents"):
            if uploaded_files:
                with st.spinner("Traitement en cours (Loader -> Split -> Embeddings)..."):
                    # √âtape 1 : Chargement
                    raw_docs = process_documents(uploaded_files)
                    
                    # √âtape 2 : Vector Store
                    st.session_state.vectorstore = build_vector_store(raw_docs)
                    
                    st.success(f"Succ√®s ! {len(raw_docs)} pages index√©es.")
            else:
                st.warning("Veuillez uploader un fichier PDF.")

    # Zone de Chat
    st.header("2. Discussion avec le cours")
    
    user_input = st.chat_input("Posez votre question ici...")

    if user_input:
        if st.session_state.vectorstore is None:
            st.warning("Veuillez d'abord traiter les documents dans la barre lat√©rale.")
        else:
            # Affichage question utilisateur
            with st.chat_message("user"):
                st.write(user_input)

            # G√©n√©ration r√©ponse
            with st.chat_message("assistant"):
                with st.spinner("R√©flexion..."):
                    rag_chain = get_rag_chain(st.session_state.vectorstore)
                    response = rag_chain.invoke({"input": user_input})
                    st.write(response["answer"])

if __name__ == "__main__":
    main()
